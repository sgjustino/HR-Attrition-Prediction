{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sgjustino/csca5622-project-supervised-ml-algorithms?scriptVersionId=152522452\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Introduction**","metadata":{}},{"cell_type":"markdown","source":"**Background: I am an Organisational Psychologist who is embarking on a journey to learn more about computer science, data science and machine learning. This notebook is part of an assignment project structured based on the project rubric requirements.**\n\n**Project Topic: Employee attrition can have significant financial and operational impacts on a company. Understanding the factors that contribute to employee attrition is essential for HR professionals and business leaders. In this analysis, we utilise various supervised machine learning algorithms like logistic regression, random forest and gradient boosting models to predict employee attrition. The dataset used in this analysis contains various features related to employee data and attrition as an outcome criterion. Through predictive models trained on the attrition dataset, I hope to understand the different approaches to machine learning better and to see how they can be translated in HR-related problems like employee attrition.**\n\n**This is a fictional dataset created by IBM data scientists to study attrition found in the link below: \nhttps://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data**\n\n**IBM. IBM HR Analytics Employee Attrition & Performance (Uploaded by Pavan Subhash) . Kaggle. Retrieved [18 Oct 2023] from [https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset/data].**","metadata":{}},{"cell_type":"markdown","source":"# **Load Packages**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, make_scorer, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-25T13:09:25.916361Z","iopub.execute_input":"2023-10-25T13:09:25.916863Z","iopub.status.idle":"2023-10-25T13:09:28.130248Z","shell.execute_reply.started":"2023-10-25T13:09:25.916816Z","shell.execute_reply":"2023-10-25T13:09:28.129209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/ibm-hr-analytics-attrition-dataset/WA_Fn-UseC_-HR-Employee-Attrition.csv')\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.132007Z","iopub.execute_input":"2023-10-25T13:09:28.133049Z","iopub.status.idle":"2023-10-25T13:09:28.206085Z","shell.execute_reply.started":"2023-10-25T13:09:28.133002Z","shell.execute_reply":"2023-10-25T13:09:28.204944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.207324Z","iopub.execute_input":"2023-10-25T13:09:28.208023Z","iopub.status.idle":"2023-10-25T13:09:28.241167Z","shell.execute_reply.started":"2023-10-25T13:09:28.207975Z","shell.execute_reply":"2023-10-25T13:09:28.240313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.243533Z","iopub.execute_input":"2023-10-25T13:09:28.243881Z","iopub.status.idle":"2023-10-25T13:09:28.324744Z","shell.execute_reply.started":"2023-10-25T13:09:28.243852Z","shell.execute_reply":"2023-10-25T13:09:28.323418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.326448Z","iopub.execute_input":"2023-10-25T13:09:28.326913Z","iopub.status.idle":"2023-10-25T13:09:28.336584Z","shell.execute_reply.started":"2023-10-25T13:09:28.326871Z","shell.execute_reply":"2023-10-25T13:09:28.335377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initial Look","metadata":{}},{"cell_type":"markdown","source":"**The fictional data is found in a .csv file (227.98 kB) on Kaggle (info in Intro).\nThere are 1470 rows of employee data with 35 columns/features (including Attrition). \nThere are 26 numerical features and 9 categorical features (including Attrition).**","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Check for missing data\nmissing_values = data.isnull().sum()\n\n# Check for constant data\nno_unique_values = data.nunique()\n\nmissing_values, no_unique_values","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.338115Z","iopub.execute_input":"2023-10-25T13:09:28.338735Z","iopub.status.idle":"2023-10-25T13:09:28.363908Z","shell.execute_reply.started":"2023-10-25T13:09:28.338701Z","shell.execute_reply":"2023-10-25T13:09:28.362867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**While there is no missing values found, 3 columns ('EmployeeCount', 'Over18', 'StandardHours') were found to have constant values for all 1470 rows. Also, 'EmployeeNumber' is a unique identifier for all 1470 rows. These 4 columns should be dropped as they would not be helpful in predicting attrition.  the categorical variables so they can be properly utilized in our models. We will also check for any class imbalance in our target variable, 'Attrition', as this will inform our approach to modeling later on.**","metadata":{}},{"cell_type":"code","source":"# Drop the 4 columns\ncolumns_to_drop = ['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeNumber']\ndata = data.drop(columns=columns_to_drop)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.366451Z","iopub.execute_input":"2023-10-25T13:09:28.366839Z","iopub.status.idle":"2023-10-25T13:09:28.374153Z","shell.execute_reply.started":"2023-10-25T13:09:28.366807Z","shell.execute_reply":"2023-10-25T13:09:28.37329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for imbalance\nattrition_counts = data['Attrition'].value_counts()\nattrition_counts","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.375345Z","iopub.execute_input":"2023-10-25T13:09:28.375745Z","iopub.status.idle":"2023-10-25T13:09:28.389647Z","shell.execute_reply.started":"2023-10-25T13:09:28.375716Z","shell.execute_reply":"2023-10-25T13:09:28.388464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nsns.countplot(x='Attrition', data=data)\nplt.title('Attrition Distribution')\nplt.xlabel('Attrition')\nplt.ylabel('Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.391132Z","iopub.execute_input":"2023-10-25T13:09:28.391696Z","iopub.status.idle":"2023-10-25T13:09:28.655767Z","shell.execute_reply.started":"2023-10-25T13:09:28.391636Z","shell.execute_reply":"2023-10-25T13:09:28.654606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As expected, the 'Attrition' column shows a significant class imbalance with 1233 'No' and 237 'Yes'. This imbalance should be addressed prior to model training to prevent overemphasis towards 'No' class when our study focus is on the employees who attrite - 'Yes'.**","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"**We will start by examining factors that are well-researched and linked to attrition - Age, Income, Job Role, Years at Company, Job Satisfaction and Work-Life Balance - and see their distribution across the two classes of Attrition. Below are some papers that are relevant if reviewers are interested.**\n\n\nAzeem, S. M., & Akhtar, N. (2014). The influence of work life balance and job satisfaction on organizational commitment of healthcare employees. International journal of human resource studies, 4(2), 18.\n\nNg, T. W., & Feldman, D. C. (2010). The relationships of age with job attitudes: A meta‐analysis. Personnel psychology, 63(3), 677-718.\n\nSteel, R. P., & Ovalle, N. K. (1984). A review and meta-analysis of research on the relationship between behavioral intentions and employee turnover. Journal of applied psychology, 69(4), 673.\n\nWright, B. E., & Christensen, R. K. (2010). Public service motivation: A test of the job attraction–selection–attrition model. International Public Management Journal, 13(2), 155-176.","metadata":{}},{"cell_type":"code","source":"# Create key features\nkey_features = ['Age', 'MonthlyIncome', 'JobRole', 'YearsAtCompany', 'JobSatisfaction', 'WorkLifeBalance']\n\nattrition_yes = data[data['Attrition'] == 'Yes']\nattrition_no = data[data['Attrition'] == 'No']\n\n# Examine distribution\nfig, axes = plt.subplots(3, 2, figsize=(20, 15))\nfig.suptitle('Distribution of Key Features by Attrition Class', fontsize=16)\n\nfor ax, feature in zip(axes.ravel(), key_features):\n    sns.histplot(attrition_yes[feature], bins=30, label='Yes', kde=True, color='red', ax=ax, alpha=0.6)\n    sns.histplot(attrition_no[feature], bins=30, label='No', kde=True, color='blue', ax=ax, alpha=0.6)\n    \n    ax.set_title(f'Distribution of {feature}', fontsize=14)\n    ax.legend()\n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()\n\n# Had to adjust for categorical data (countplot) to make better sense\n# Side note: Psychologists treat likert scale data like JobSat as interval data\n\nplt.figure(figsize=(15, 7))\nsns.countplot(data=data, y='JobRole', hue='Attrition', palette='viridis')\nplt.title('Count of Attrition in Different Job Roles', fontsize=16)\nplt.xlabel('Count', fontsize=13)\nplt.ylabel('Job Role', fontsize=13)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:28.660408Z","iopub.execute_input":"2023-10-25T13:09:28.661423Z","iopub.status.idle":"2023-10-25T13:09:32.792641Z","shell.execute_reply.started":"2023-10-25T13:09:28.661377Z","shell.execute_reply":"2023-10-25T13:09:32.791376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initial look at the distribution does suggest that the data is inclined towards research-backed evidences. There is a noticeable difference between employees who left and those who stayed for features like Age, Monthly Income and Years at Company.**\n\n**However, the imbalanced class distribution does make it hard to visualise the difference for JobSat and WLB. Although these are the factors that have the most findings on negative engagement, abesenteeism and attrition in organisational studies.**\n\n**On the other hand, some job roles experience more attrition (ratio wise) than other positions - notably with Sales Executive, Research Scientist, Laboratory Technician, Sales Representative - and less in more management roles - notably Director and Manager level job roles. The Job Level feature might have been a better representation visually but I think the Job Role will be able to help us make sense of the data qualitatively better.**\n\n**Next, we will encode the categorical variables and construct a correlation matrix to understand the relationships of the factors better and identify any multicollinearity issues.**","metadata":{}},{"cell_type":"code","source":"# Features and target\nX = data.drop(columns=['Attrition'])\ny = data['Attrition']\n\n# Encoding categorical variables\ncategorical_cols = X.select_dtypes(include=[object]).columns.tolist()\n\nX = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n\nX = X.astype(int)\n\n#Encode Attrition\ny = LabelEncoder().fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:32.794075Z","iopub.execute_input":"2023-10-25T13:09:32.794431Z","iopub.status.idle":"2023-10-25T13:09:32.814922Z","shell.execute_reply.started":"2023-10-25T13:09:32.794402Z","shell.execute_reply":"2023-10-25T13:09:32.813316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation Matrix\ncorrelation_matrix = X.corr()\n\nplt.figure(figsize=(30, 20))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, \n            cbar_kws={\"shrink\": .75}, linewidths=.5)\nplt.title('Correlation Matrix of Features', fontsize=16)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:32.816122Z","iopub.execute_input":"2023-10-25T13:09:32.817262Z","iopub.status.idle":"2023-10-25T13:09:38.561394Z","shell.execute_reply.started":"2023-10-25T13:09:32.817224Z","shell.execute_reply":"2023-10-25T13:09:38.560088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Between the predictors, 'MonthlyIncome' is highly correlated with 'JobLevel' (0.95), suggesting that these variables might be conveying overlapping information. 'TotalWorkingYears' has strong positive correlations with 'JobLevel' (0.78) and 'MonthlyIncome' (0.77), which is expected as more experienced employees are likely at a higher job level and earn more. 'YearsWithCurrManager', 'YearsAtCompany', and 'YearsInCurrentRole' are also positively correlated, indicating that employees tend to stay in their roles and with the same managers.**\n\n**For this analysis,I will remove MonthlyIncome,TotalWorkingYears, YearsInCurrentRole and YearsWithCurrManager using a cutoff of 0.7 correlation coefficient. This will retain JobLevel and YearsAtCompany and remove possibility of multicollinearity from the features.**\n\n**For MonthlyIncome, the high correlation with job level and sensitivity around salary makes it both redundant and hard to work with in policy change management. For TotalWorkingYears, the data will be encapsulated in other features like YearsAtCompany. For YearsInCurrentRole and YearsWithCurrManager, the data will not pose much additional insights beyond YearsAtCompany, considering organisational structures/functional specialisations (e.g. Accounting) are what keep individuals in similar Department and role.**","metadata":{}},{"cell_type":"code","source":"X = X.drop(columns=['MonthlyIncome','TotalWorkingYears','YearsInCurrentRole','YearsWithCurrManager'])\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:38.563167Z","iopub.execute_input":"2023-10-25T13:09:38.564288Z","iopub.status.idle":"2023-10-25T13:09:38.572007Z","shell.execute_reply.started":"2023-10-25T13:09:38.564231Z","shell.execute_reply":"2023-10-25T13:09:38.57097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"# Recap of changes to feature data\nX.info()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:38.573259Z","iopub.execute_input":"2023-10-25T13:09:38.574138Z","iopub.status.idle":"2023-10-25T13:09:38.598775Z","shell.execute_reply.started":"2023-10-25T13:09:38.574105Z","shell.execute_reply":"2023-10-25T13:09:38.597815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**\n\n**We will first try to build a logistic regression model, which is specifically designed for binary classification problems and the most straightforward model in our case. Prior to model building, we will split the data into training and test dataset and balance the data using SMOTE (Synthetic Minority Over-sampling Technique) due to the highly imbalanced class distribution in Attrition.**\n\n**In balancing, there are a couple of techniques commonly employed - weighting, down-sampling, up-sampling and SMOTE.\nHere, we will utilise SMOTE to combine both down-sampling of the majority class (Attrition - 'No') and up-sampling of the minority class (Attrition - 'Yes').**","metadata":{}},{"cell_type":"code","source":"# Split the data into training/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Balance data\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:38.600371Z","iopub.execute_input":"2023-10-25T13:09:38.600862Z","iopub.status.idle":"2023-10-25T13:09:38.639847Z","shell.execute_reply.started":"2023-10-25T13:09:38.600816Z","shell.execute_reply":"2023-10-25T13:09:38.638266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression model\nlogistic_model = LogisticRegression(solver='liblinear')\nlogistic_model.fit(X_train_resampled, y_train_resampled)\n\ny_pred = logistic_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)\n\nypp = logistic_model.predict_proba(X_test)[:, 1]\n\nfpr, tpr, th = roc_curve(y_test, ypp) \nauc = roc_auc_score(y_test, ypp)\n\nlw = 2\nplt.plot(fpr, tpr, color='blue', lw=lw, label='AUC = %0.2f' % auc)\nplt.plot([0, 1], [0, 1], color='red', lw=lw, linestyle='--') \nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"AUC-ROC Score:\", auc)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:38.642185Z","iopub.execute_input":"2023-10-25T13:09:38.642698Z","iopub.status.idle":"2023-10-25T13:09:39.074412Z","shell.execute_reply.started":"2023-10-25T13:09:38.642642Z","shell.execute_reply":"2023-10-25T13:09:39.073251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Using the balanced sample via SMOTE, the logistic regression model produced an F1 score of 0.51 and an AUC of 0.81. Considering the small number of employees who left the company, this suggest that the linear model was surprisingly effective in predicting attrition. When dealing with imbalanced datasets, it is perhaps useful to point out that the F1 score can be a better metric than AUC. This is because F1 score balances precision and recall and is less affected by class imbalance. It is also important to look across the results and examine relevance to the study. In our case, the accuracy score is less useful as there is lesser emphasis on the minority class (Attrition - 'Yes') with the better results in the majority class.**\n\n**To improve the results, we will seek to perform hyperparameter tuning for the logistic regression model via GridSearchCV (with Cross Validation = 5) on the SMOTE balanced dataset.**","metadata":{}},{"cell_type":"code","source":"# Hyperparameter Tuning\nparam_grid_lr = {\n    'C': [0.001, 0.01, 0.1, 1, 10],  \n    'penalty': ['l1', 'l2'],   \n}\n\n# GridSearchCV\ngrid_search_lr = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=1000), param_grid_lr, cv=5, n_jobs=-1)\ngrid_search_lr.fit(X_train_resampled, y_train_resampled)\n\nbest_params_lr = grid_search_lr.best_params_\nbest_estimator_lr = grid_search_lr.best_estimator_\nbest_estimator_lr.fit(X_train_resampled, y_train_resampled)\n\ny_pred_tuned_lr = best_estimator_lr.predict(X_test)\naccuracy_tuned_lr = accuracy_score(y_test, y_pred_tuned_lr)\nconf_matrix_tuned_lr = confusion_matrix(y_test, y_pred_tuned_lr)\nclass_report_tuned_lr = classification_report(y_test, y_pred_tuned_lr)\n\nprint(\"\\nBest Parameters (Logistic Regression):\", best_params_lr)\nprint(\"Accuracy (Tuned Logistic Regression):\", accuracy_tuned_lr)\nprint(\"Confusion Matrix (Tuned Logistic Regression):\\n\", conf_matrix_tuned_lr)\nprint(\"Classification Report (Tuned Logistic Regression):\\n\", class_report_tuned_lr)\n\n# AUC-ROC\ny_prob_lr = best_estimator_lr.predict_proba(X_test)[:, 1]\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, y_prob_lr)\nroc_auc_lr = roc_auc_score(y_test, y_prob_lr)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'AUC = {roc_auc_lr:.2f}')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()\n\nprint(\"AUC-ROC Score:\", roc_auc_lr)","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:39.076061Z","iopub.execute_input":"2023-10-25T13:09:39.076381Z","iopub.status.idle":"2023-10-25T13:09:43.221681Z","shell.execute_reply.started":"2023-10-25T13:09:39.076353Z","shell.execute_reply":"2023-10-25T13:09:43.220263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**With the hyperparameter tuning, it seems that there is only a slight improvement in the F1 score from 0.51 to 0.52 for Attrition['Yes] and AUC from 0.811 to 0.812. Nonetheless, with the relatively small sample and lack of longitudinal employee data, such scores are decent in aiding the company to predict Attrition and work on tailored interventions.**\n\n**We will seek to examine the data through a different model - Random Forest. Considering the numerous correlations across the features, the low F1 score in the logistic regression may be attributed to more non-linear and complex relationships across features and Attrition. There may also be more noisy data in this case as our hyperparameter tuning did not pose much benefit to the linear regression. This prompt us to utilise the Random Forest model to see if we can deal better with the possible presence of complex relationships and noisy data.**\n\n**We will jump straight into hyperparameter tuning via GridSearchCV (with Cross Validation = 5) on the SMOTE balanced dataset for Random Forest model building.**","metadata":{}},{"cell_type":"code","source":"# Random Forest\n# Hyperparameter Tuning\nparam_grid_rf = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10]\n}\n\n# GridSearchCV\ngrid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=5, n_jobs=-1)\ngrid_search_rf.fit(X_train_resampled, y_train_resampled)\nbest_params_rf = grid_search_rf.best_params_\nbest_estimator_rf = grid_search_rf.best_estimator_\nbest_estimator_rf.fit(X_train_resampled, y_train_resampled)\n\ny_pred_tuned_rf = best_estimator_rf.predict(X_test)\naccuracy_tuned_rf = accuracy_score(y_test, y_pred_tuned_rf)\nconf_matrix_tuned_rf = confusion_matrix(y_test, y_pred_tuned_rf)\nclass_report_tuned_rf = classification_report(y_test, y_pred_tuned_rf)\n\nprint(\"\\nBest Parameters (Random Forest):\", best_params_rf)\nprint(\"Accuracy (Tuned Random Forest):\", accuracy_tuned_rf)\nprint(\"Confusion Matrix (Tuned Random Forest):\\n\", conf_matrix_tuned_rf)\nprint(\"Classification Report (Tuned Random Forest):\\n\", class_report_tuned_rf)\n\n# AUC-ROC\ny_prob_rf = best_estimator_rf.predict_proba(X_test)[:, 1]\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_prob_rf)\nroc_auc_rf = roc_auc_score(y_test, y_prob_rf)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'AUC = {roc_auc_rf:.2f}')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()\n\nprint(\"AUC-ROC Score (Random Forest):\", roc_auc_rf)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:09:43.224236Z","iopub.execute_input":"2023-10-25T13:09:43.225093Z","iopub.status.idle":"2023-10-25T13:10:22.763886Z","shell.execute_reply.started":"2023-10-25T13:09:43.225049Z","shell.execute_reply":"2023-10-25T13:10:22.76269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Interestingly, the hyperparameter tuned Random Forest model did not perform as well as the logistic regression with a F1 score of 0.36 for Attrition['Yes] and an AUC score of 0.77.**\n\n**While we hypothesized more complex and non-linear relationships across features and Attrition, the relatively poor performance of Random Forest in this context suggests that the underlying data may not exhibit strong non-linear dependencies.**\n\n**Next, we will seek to improve the prediction modelling through Gradient Boosting method. As GB builds trees sequentially by weighing misclassified samples more heavily, it is possible that the model will provide higher predictive stats for Attrition.**\n\n**Again, we will jump straight into hyperparameter tuning via GridSearchCV (with Cross Validation = 5) on the SMOTE balanced dataset for Gradient Boosting model building.**","metadata":{}},{"cell_type":"code","source":"# Gradient Boosting\n# Hyperparameter Tuning\nparam_grid_gb = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.1, 0.2],\n    'max_depth': [3, 4, 5]\n}\n\n# GridSearchCV\ngrid_search_gb = GridSearchCV(estimator=GradientBoostingClassifier(random_state=42),param_grid=param_grid_gb,\n                cv=5, n_jobs=-1)\ngrid_search_gb.fit(X_train_resampled, y_train_resampled)\nbest_params_gb = grid_search_gb.best_params_\nbest_estimator_gb = grid_search_gb.best_estimator_\nbest_estimator_gb.fit(X_train_resampled, y_train_resampled)\n\ny_pred_tuned_gb = best_estimator_gb.predict(X_test)\naccuracy_tuned_gb = accuracy_score(y_test, y_pred_tuned_gb)\nconf_matrix_tuned_gb = confusion_matrix(y_test, y_pred_tuned_gb)\nclass_report_tuned_gb = classification_report(y_test, y_pred_tuned_gb)\n\nprint(\"\\nBest Parameters (Gradient Boosting):\", best_params_gb)\nprint(\"Accuracy (Tuned Gradient Boosting):\", accuracy_tuned_gb)\nprint(\"Confusion Matrix (Tuned Gradient Boosting):\\n\", conf_matrix_tuned_gb)\nprint(\"Classification Report (Tuned Gradient Boosting):\\n\", class_report_tuned_gb)\n\n# AUC-ROC\ny_prob_gb = best_estimator_gb.predict_proba(X_test)[:, 1]\nfpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test, y_prob_gb)\nroc_auc_gb = roc_auc_score(y_test, y_prob_gb)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr_gb, tpr_gb, color='blue', lw=2, label=f'AUC = {roc_auc_gb:.2f}')\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('ROC Curve')\nplt.legend(loc='lower right')\nplt.show()\n\nprint(\"AUC-ROC Score (Gradient Boosting):\", roc_auc_gb)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:10:22.765335Z","iopub.execute_input":"2023-10-25T13:10:22.765717Z","iopub.status.idle":"2023-10-25T13:11:34.488916Z","shell.execute_reply.started":"2023-10-25T13:10:22.765685Z","shell.execute_reply":"2023-10-25T13:11:34.487698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Again, the hyperparameter tuned Gradient Boosting model did not perform as well as the logistic regression with a F1 score of 0.42 for Attrition['Yes] and an AUC score of 0.78. It did, however, do better than the Random Forest model, likely due to the sequential learning algorithm. Ensemble methods tend to excel with non-linear relationships and interactions between features. The relatively poor performance of Random Forest and Gradient Boosting in this context suggests that the underlying data may indeed not be exhibiting strong non-linear dependencies.**\n\n**With the analysis of this binary classification problem through 3 different models and hyperparameter-tuning, we will look at the results collectively in the next section.**","metadata":{}},{"cell_type":"markdown","source":"# Results & Analysis","metadata":{}},{"cell_type":"code","source":"models = [best_estimator_lr, best_estimator_rf, best_estimator_gb]\nmodel_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']\n\nplt.figure(figsize=(10, 6))\nauc_scores = []\nprecision_scores = []\nrecall_scores = []\nf1_scores = []\n\n# ROC\nfor model, name in zip(models, model_names):\n    y_prob = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n    fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label=1)\n    auc = roc_auc_score(y_test, y_prob)\n    auc_scores.append(auc)\n    \n    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})')\n    y_pred = model.predict(X_test)\n    precision = precision_score(y_test, y_pred, pos_label=1)\n    recall = recall_score(y_test, y_pred, pos_label=1)\n    f1 = f1_score(y_test, y_pred, pos_label=1)\n\n    precision_scores.append(precision)\n    recall_scores.append(recall)\n    f1_scores.append(f1)\n\n# Baseline\nplt.plot([0, 1], [0, 1], 'k--', label='Baseline')\n\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.title('AUC-ROC Curve')\nplt.legend()\nplt.show()\n\n# Consolidated Results\nmetrics_df = pd.DataFrame({\n    'Model': model_names,\n    'AUC': auc_scores,\n    'Precision': precision_scores,\n    'Recall': recall_scores,\n    'F1 Score': f1_scores\n})\n\nprint(\"\\nModel Evaluation Metrics:\")\nprint(metrics_df)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:11:34.490772Z","iopub.execute_input":"2023-10-25T13:11:34.491123Z","iopub.status.idle":"2023-10-25T13:11:34.951805Z","shell.execute_reply.started":"2023-10-25T13:11:34.491093Z","shell.execute_reply":"2023-10-25T13:11:34.950581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looking at the data collectively, we can tell again that the logistic regression model emerged as the top performer, achieving an F1 score of approximately 0.52 for Attrition['Yes'] and an AUC of 0.81. This outcome suggests that there is likely the existence of linear relationships between Attrition and the features and Attrition is less likely to depend on non-linear or more complex relationships with other features. Also, when dealing with imbalanced datasets, it is important to point out again that the F1 score can be a better metric than AUC. This is because F1 score balances precision and recall and is less affected by class imbalance. While accuracy score is more commonly used, it is less useful in this context as there is lesser emphasis on the minority class (Attrition - 'Yes') due to the overemphasis of better results in the majority class. Overall, considering the relatively small sample of Attrition['Yes'] data and lack of longitudinal employee data, the results from the logistic regression model is satisfactory and decent in aiding the company to predict Attrition and work on tailored interventions.**\n\n**On the other hand, Random Forest and Gradient Boosting algorithms may not have shined here because of the relatively small/moderate sized dataset. For these 2 ensemble methods, it is likely that we will see better predictive statistics when there is a larger dataset in order to fully exploit their capabilities. In our context, their complexity in building the model might have led to overfitting issues with limited data size.**","metadata":{}},{"cell_type":"code","source":"# Examining top 10 Features\ncoefficients_lr = best_estimator_lr.coef_[0]\nimportance_df_lr = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients_lr})\nimportance_df_lr = importance_df_lr.sort_values(by='Coefficient', ascending=False)\ntop_10_features_lr = importance_df_lr.head(10)\n\nprint(top_10_features_lr)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncoefficients_lr = best_estimator_lr.coef_[0]\nimportance_df_lr = pd.DataFrame({'Feature': X_train.columns, 'Coefficient': coefficients_lr})\nimportance_df_lr = importance_df_lr.sort_values(by='Coefficient', ascending=False)\ntop_10_features_lr = importance_df_lr.head(10)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Coefficient', y='Feature', data=top_10_features_lr, palette='viridis')\nplt.xlim(0, top_10_features_lr['Coefficient'].max())  # Set the x-axis limit from 0 to the maximum coefficient\nplt.xlabel('Coefficient')\nplt.ylabel('Feature')\nplt.title('Top 10 Important Features (Logistic Regression)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T13:11:34.953265Z","iopub.execute_input":"2023-10-25T13:11:34.954159Z","iopub.status.idle":"2023-10-25T13:11:35.266928Z","shell.execute_reply.started":"2023-10-25T13:11:34.954121Z","shell.execute_reply":"2023-10-25T13:11:35.265603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Discussion and Conclusion","metadata":{}},{"cell_type":"markdown","source":"**As the logistic regression model emerged as the winner in our dataset context, we plotted the top 10 features from the model for the prediction on attrition. Below, we listed some explanation for each of the feature's possible causality in attributing attrition. We will also leave out HourlyRate and MonthlyRate due to their relatively small coefficient which reflects little to no influence vis-a-vis other features on Attrition in real-world settings.**\n\n- **OverTime_Yes (1.509276): Overtime work may lead to burnout and decreased job satisfaction, which can contribute to attrition.**\n\n- **PerformanceRating (0.417478): Surprisingly, better performance rating is associated with a higher likelihood of attrition. This, however, suggest that top employees may have attrited in light of better prospects for their careers elsewhere.**\n\n- **BusinessTravel_Travel_Frequently (0.194502): Frequent travel may disrupt work-life balance, possibly contributing to attrition.**\n\n- **NumCompaniesWorked (Coefficient: 0.140618): Employees who have worked in more companies may be more willing to seek outside opportunities in line with their working history.**\n\n- **YearsSinceLastPromotion (0.090782): Lack of career advancement may lead to employee dissatisfaction and contribute to attrition.**\n\n- **Gender_Male (0.071276): Male employees are slightly more likely to attrite than female employees. This has no working research explanation and would warrant further investigation in the company. However, taking a look across employee data might suggest that males may have a higher tendency to score higher on the other relevant features in affecting attrition.**\n\n- **MaritalStatus_Single (0.048594): The lack of family commitment or other financial ties might have influence single employees to look for other career opportunies elsewhere.**\n\n- **DistanceFromHome (Coefficient: 0.034816): Commuting stress or a desire for proximity to home may play a role in influencing attrition.**\n\n**In this analysis, the results suggest that addressing work-life balance issues related to overtime and frequent business travel may help reduce attrition rates. Additionally, HR departments should pay attention to employees who have not received promotions for an extended period, as this group exhibits a higher likelihood of attrition. Higher performing employees are also likely to be talent-scouted by competition. A focus on recognition of work and tangible rewards should be emphasized in performance ranking systems, while employee engagement initiatives and personalised retention strategies may be useful through predicting potential employees who are thinking of leaving. Care has to be taken, however, in how HR policies are implemented to avoid the case of over-classification or targeting of employees on a basis on higher likelihood to leave the company from past data.**\n\n**For the model building, it is interesting that the more simple logistic regression emerged as a winner as compared with more powerful and complex models. This also bring about the importance of starting with simple models and the linkage to Occam's razor principle. In this exercise, I had learnt more about the different modeling approaches and also see how the translation of model building intertwine with the necessity to understand the real-world business question through employee data and attrition. Looking at the study approach, it is possible that the selected features, while important individually, may not capture the full complexity of attrition patterns in the organization. Attrition often results from a combination of factors, including personal preferences, job satisfaction, and career advancement opportunities, which may not be fully represented in the available data. Patterns on attrition should also be viewed longitudinally, as the multi-year experiences of employees will have a larger influence on their decision to leave the company.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}